# -*- coding: utf-8 -*-
"""project_step2_model_visualizaruin_kfold.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pq4JdBRoCkXbmwBR8F0Ru_hc-s-Xf0rQ
"""

!pip install colorama
!pip install rarfile
from google.colab import files
import rarfile
import os
from colorama import init, Fore, Back, Style

import seaborn as sns
import matplotlib.pyplot as plt


import tensorflow as tf
import numpy as np
import sklearn 
import random
import pandas as pd

from sklearn.preprocessing import Normalizer , StandardScaler
from sklearn.preprocessing import LabelEncoder , OneHotEncoder


from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D , Conv1D
from tensorflow.keras.layers import LSTM , TimeDistributed
from tensorflow.keras.layers import AveragePooling2D ,AveragePooling1D, MaxPooling1D , MaxPool2D , Input
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Dense , Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense,RepeatVector, LSTM, Dropout
from tensorflow.keras.layers import Bidirectional, Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.metrics import Accuracy , Precision , Recall

from imblearn.over_sampling import SMOTE , ADASYN , RandomOverSampler , SMOTENC , BorderlineSMOTE , SVMSMOTE ,  KMeansSMOTE
from imblearn.combine import SMOTEENN , SMOTETomek


import keras
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
import time
from numpy import expand_dims

def Df_Builder(path , show_info=True):
  df = pd.read_csv(path , index_col='Time')
  df = df[40:]
  df.index = pd.to_datetime(df.index)
  if show_info :
    print(df['Label'].value_counts())
    print(df.shape)
  return df





def Df_Separation(df):

  x = df.iloc[: , :-1]
  y = df.iloc[: , -1]

  print(f'shape x : {x.shape}')
  print(f'shape y : {y.shape}')

  return x,y







def Df_Normalization(x,y):


  x_normalized = standardize.fit_transform(x)
  y_encoded = encoder.fit_transform(y)

  return x_normalized,y_encoded








def Df_Reshape(x_normalized ):

  x_series = x_normalized.reshape((x_normalized.shape[0], x_normalized.shape[1] ,1))
  print(f'x = reshaped : {x_series.shape}')


  return x_series







'''---------------------------------------------------------------------'''
def Prepare_Finel_Test(path , start_point , end_point):
  df = Df_Builder(path , show_info = False)
  df = df[df.index >= start_point]
  df = df[df.index <= end_point]
  x = df.iloc[: , :-1]
  y = df.iloc[: , -1]
  # df.drop('Label' , inplace=True , axis=1)

  TEST = standardize.fit_transform(x)
  TEST_y = y

  TEST = TEST.reshape((TEST.shape[0], TEST.shape[1] ,1))
  print(len(TEST_y))
  print((TEST.shape))
  return TEST,TEST_y





def Predict_best_model(predictions , acc , counts ):
  acc_arr = np.array(acc)
  acc_arr = acc_arr.reshape((counts,3))
  acc_df = pd.DataFrame(acc_arr)
  acc_df['subtraction'] = acc_df.iloc[:,0]-acc_df.iloc[:,1]
  minimum = min(acc_df['subtraction'])
  ind = acc_df[acc_df['subtraction'] == minimum]
  best_pred = predictions[int(ind.index.values)]
  print(f'selected model : {int(ind.index.values)}')
  return best_pred






def Model_builder2(dropout_rate=0.3 , input_size=(36,1) , activation_func = 'relu' , LSTM_units = 32 , Dense_units = 32):
  model = Sequential()
  model.add(Conv1D(filters=8, kernel_size=36, activation=activation_func, input_shape=input_size))
  model.add(Dropout(dropout_rate))
  model.add(Conv1D(filters=16, kernel_size=1, activation=activation_func))
  model.add(Dropout(dropout_rate))
  model.add(Flatten())
  model.add(RepeatVector(2))
  model.add(LSTM(LSTM_units, activation=activation_func))
  model.add(Dense(Dense_units, activation=activation_func))
  model.add(Dropout(dropout_rate))
  model.add(Dense(2 , activation='softmax'))
  model.compile(loss='sparse_categorical_crossentropy', 
                optimizer=optimizer, 
                metrics=['acc'])
  # print(model.summary())
  return model








def plot_acc(acc,name,counts):
  plt.style.use('dark_background')
  print("columns in order left to right : train , validation , test accuracies")
  print("rows : steps")
  arr = np.array(acc)
  arr = arr.reshape((counts,3))
  plt.plot(arr[:,0] , color='red' , linewidth=3 , label='Train')
  plt.plot(arr[:,1] , color='blue'  , linewidth=3 , label='Validation')
  plt.plot(arr[:,2] , color='green'  , linewidth=3 , label='Test')
  plt.title(f'Accuracy per steps - {name}')
  plt.xlabel('#steps')
  plt.ylabel('acc')
  plt.legend()
  plt.show()






def Df_Accuracies(acc , name,counts):
  plt.style.use("dark_background")
  arr = np.array(acc)
  arr = arr.reshape((counts,3))
  steps = [i for i in range(counts)]


  result_df = pd.DataFrame({'steps' : steps ,'acc train' : arr[:,0],'acc validation' : arr[:,1],'acc test' : arr[:,2]})

  result_df.index = result_df['steps']
  result_df = result_df.drop(['steps'] , axis=1)


  f , ax = plt.subplots(figsize=(5,counts))

  sns.heatmap(result_df,
              cmap='RdGy' , 
              annot=True ,
              annot_kws={'size' : 15},
              cbar=False)
  plt.title(name , size = 20)
  plt.xticks(size=15 ,rotation=20 )
  plt.yticks(size=15 ,rotation=20 )
  plt.show()







def Plot_model(historyList , folds):
  plt.style.use("dark_background")
  f , ax = plt.subplots(figsize=(8,folds*4) , nrows=folds , ncols =2)

  for history,i in zip(historyList , range(folds)):

      ax[i][0].plot(history.history['acc'])
      ax[i][0].plot(history.history['val_acc'])
      ax[i][0].set_title('model accuracy')
      ax[i][0].set_ylabel('accuracy')
      ax[i][0].set_xlabel('epoch')
      ax[i][0].legend(['train', 'valid'], loc='upper left')
      # summarize history for loss
      ax[i][1].plot(history.history['loss'])
      ax[i][1].plot(history.history['val_loss'])
      ax[i][1].set_title('model loss')
      ax[i][1].set_ylabel('loss')
      ax[i][1].set_xlabel('epoch')
      ax[i][1].legend(['train', 'valid'], loc='upper left')
  plt.show()









def Prepare_Report(acc,counts):
  arr = np.array(acc)
  arr = arr.reshape((counts,3))
  avg_acc =np.mean(arr , axis=0)
  return avg_acc

upload = files.upload()


date_path = "/content/date.xlsx"




def Build_Labels_DF(index_path , date_path ,prediction):

  path = date_path
  date = pd.read_excel(path)
  date['Date'] = pd.to_datetime(date['Date'] ,format= "%Y%m%d").dt.date


  df = pd.read_csv(index_path)
  df.index = df['Time']
  df = df[df.index >= start_point]
  df = df[df.index <= end_point]

  df['prediction'] = prediction
  df['prediction'][start_point]


  label = list()
  for i in date['Date'] :
    if str(i) not in df['Time']:
      label.append(0)
    else :
      label.append(df['prediction'][str(i)])


  assert (
      pd.value_counts(label)[0] == len(date['Date']) - len(df['Time']) and
      pd.value_counts(label)[1] == pd.value_counts(prediction)[1] and
      pd.value_counts(label)[-1] == pd.value_counts(prediction)[-1]
      ) ,  "there are a problem with labels"

  date['Label'] = label
  date.index = date['Date']
  date.drop('Date' , axis=1 ,inplace=True)
  return date

def outputIndexes(model):
  Cnn_outputs = []
  Lstm_outputs = []

  for i in range(len(model.layers)):
    layer = model.layers[i]
    if 'conv' in layer.name :
      Cnn_outputs.append(i)
      # print(i, layer.name, layer.output.shape)

    elif 'lstm' in layer.name :
      Lstm_outputs.append(i)
      # print(i, layer.name, layer.output.shape)

  return Cnn_outputs,Lstm_outputs






def visualizeConvs(model , x , y , output_index):

  model = keras.Model(inputs=model.inputs, outputs=model.layers[output_index].output)
  img = x
  feature_maps = model.predict(img)
  sh = feature_maps.reshape((feature_maps.shape[0],feature_maps.shape[2]))
  sh = sh[:sh.shape[1] , :sh.shape[1]]
  print(sh.shape)
  plt.axis('off')
  plt.imshow(sh)
  plt.show()




def visualizeLstm(model , x , y , output_index):
  print("lstm feature map")
  model = keras.Model(inputs=model.inputs, outputs=model.layers[output_index].output)
  img = x
  feature_maps = model.predict(img)
  plt.plot(feature_maps)
  plt.show()
  print('-----------')




def plotLSTM(model , x , y ):
    _ , outputs = outputIndexes(model)
    for output in outputs :
      visualizeLstm(model , x , y , output)
    plt.show()


def plotCNN(model , x , y ):
    outputs , _ = outputIndexes(model)
    for output in outputs :
      visualizeConvs(model , x , y , output)
    plt.show()

class SplitTimeSeries():
    def __init__(self, n_splits):
        self.n_splits = n_splits
    
    def split(self, X, y=None, groups=None):
        n_samples = X
        k_fold_size = n_samples // self.n_splits

        indices = np.arange(n_samples)
        margin = 0
        start = indices[0]
        for i in range(self.n_splits):
            i += 1 
            stop = k_fold_size * (i)
            divsion = int(0.9 * (stop))
            yield indices[start], indices[divsion] , indices[stop-1]

def Training(path,counts = 10,k=3):
  
  acc = list()
  models_list = []
  predictions = []
  name = path.split('/')[-1]
  name = name.split('.')[0]
  print(Fore.WHITE)
  print(f'---------------------------------------------------------------{name}------------------------------------------------------------')

## calling index
  df = Df_Builder(path)

  x_train,y_train = Df_Separation(df)
  TEST_x,TEST_y = Prepare_Finel_Test(path , start_point , end_point)




## loop over main iterations
  for i in range(counts): 
    print(Fore.MAGENTA +  f'\n.................................................................')
    print(Fore.MAGENTA +  f'........................... step {i} starting ...........................')
    print(Fore.MAGENTA +  f'.................................................................\n')
    print(Fore.WHITE)

    Folds_acc_train = list()
    Folds_acc_valid = list()
    Folds_acc_test = list()
    historyList = list()




## loop over k of kfold
    for j,splits in enumerate(SplitTimeSeries(n_splits=k).split(X=x_train.shape[0])) :

## find indices of each kfold
        start , mid , end = splits
        print(Fore.LIGHTYELLOW_EX +   f'\n.................. fold {j} starting ..................')
        print(Fore.WHITE)

## data preparation
        print("fold data indices:" , "start:",start ,"end:",end)
        xTrain =  x_train[start:mid]
        yTrain =  y_train[start:mid]
        xValid =  x_train[mid:end]
        yValid =  y_train[mid:end]


        x_train_normalized,y_train_encoded = Df_Normalization(xTrain,yTrain)
        x_valid_normalized,y_valid_encoded = Df_Normalization(xValid,yValid)

        if i==0 : print(Fore.RED + f'xtrain normalized : \n{x_train_normalized}')
        print(Fore.WHITE)


        x_train_series = Df_Reshape(x_train_normalized)
        x_valid_series = Df_Reshape(x_valid_normalized)

    
## train model
        model = Model_builder2()
        history = model.fit(x_train_series, y_train_encoded, 
                          epochs=epochs , batch_size=batch_size ,
                          validation_data = ( x_valid_series , y_valid_encoded)
                          , verbose = 0 , callbacks=[callback])
        historyList.append(history)
        



## predict test
        predict = model.predict(TEST_x)
        predict = predict.argmax(axis=1)
        predict = np.where(predict==0 , -1 , 1)

### accuracy calculation
        train_loss, train_accuracy = model.evaluate(x_train_series, y_train_encoded , verbose=0)
        valid_loss, valid_accuracy = model.evaluate(x_valid_series, y_valid_encoded  , verbose=0)
        main_test_accuracy = sklearn.metrics.accuracy_score(predict, TEST_y)

        
        print(Fore.WHITE +  f"train iteration:{i} - fold:{j}  ----->  {train_accuracy}")
        print(Fore.WHITE +  f"valid iteration:{i} - fold:{j}  ----->  {valid_accuracy}")
        print(Fore.WHITE +  f"test iteration:{i} - fold:{j}  ----->  {main_test_accuracy}")





## add accuracy to seperated lists
        Folds_acc_train.append(train_accuracy)
        Folds_acc_valid.append(valid_accuracy)
        Folds_acc_test.append(main_test_accuracy)




## visualize cnn output
        plotCNN(model=model , x=x_train_normalized , y=y_train_encoded)
        plotLSTM(model=model , x=x_train_normalized , y=y_train_encoded)


## hold predicted labels of last kfold
        if j==k-1 :
          predictions.append(list(predict))


## plot model training
    Plot_model(historyList=historyList , folds=k)



## average of accuracies over k 
    avg_Fold_acc_train = sum(Folds_acc_train)/len(Folds_acc_train)
    avg_Fold_acc_test = sum(Folds_acc_test)/len(Folds_acc_test)
    avg_Folds_acc_valid = sum(Folds_acc_valid)/len(Folds_acc_valid)


## saving all averaged acciracies
    acc.append(avg_Fold_acc_train)
    acc.append(avg_Fold_acc_test)
    acc.append(avg_Folds_acc_valid)
    print("\n\n\n")






## plot line plot of accuracies trend
  plot_acc(acc,name , counts)
  plt.show()
  # time.sleep(5)

## average of whole accueacies over main iterations(counts)
  print(Fore.WHITE + f"---------- Report ---------- \n")
  mean_train_acc , mean_validation_acc , mean_test_acc = Prepare_Report(acc , counts)
  print(Fore.WHITE + f'mean train accueacies over {counts} model training ------ >> {mean_train_acc}')
  print(Fore.WHITE + f'mean validation accuracies over {counts} model training ------ >> {mean_validation_acc}')
  print(Fore.WHITE + f'mean test accuracies over {counts} model training ------ >> {mean_test_acc}')

## plot table of accuracies in detail
  Df_Accuracies(acc , name , counts)


## find best model and return labels of test data of this model
## based on mean of kfolds over n iteration best model will be chosen (its label prdictions are based on last fold)
  best_pred = Predict_best_model(predictions ,acc , counts)




## retrun best predicted labels
  return best_pred , TEST_y

upload = files.upload()

epochs = 100
batch_size = 128
counts = 10
initial_learning_rate = 0.1
decay_rate = 0.9
patience = 15
start_point = "2019-03-25"
end_point = "2022-03-19"


standardize = StandardScaler()
encoder = LabelEncoder()
callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=patience)
lr_schedule = ExponentialDecay(initial_learning_rate,decay_steps=100000,decay_rate=decay_rate,staircase=True)
optimizer = SGD(learning_rate=lr_schedule) 



zip_path = '/content/indicators_9rar.rar'
unzip_path = '/content/test'


with rarfile.RarFile(zip_path , 'r') as unrar :
  unrar.extractall(unzip_path)


files = os.listdir(path=unzip_path)
file_paths = [os.path.join(unzip_path , f_name) for f_name in files]
file_paths

model = Model_builder2()
model.summary()
model.layers

"""### Initialization"""

file_paths

predictionsshebhern , TestYshebhern  = Training(file_paths[2] , counts=6 , k=3)
shebhern = Build_Labels_DF(file_paths[2] , date_path ,predictionsshebhern)

predictionskachad  , TestYkachad = Training(file_paths[4] , counts=6 , k=3)
kachad = Build_Labels_DF(file_paths[4] , date_path ,predictionskachad)

predictionsfammeli ,TestYfammeli = Training(file_paths[0] , counts=6 , k=3)
fammeli = Build_Labels_DF(file_paths[0] , date_path ,predictionsfammeli)

predictionsvebemellat ,TestYvebemellat = Training(file_paths[7] , counts=6 , k=3)
vebemellat = Build_Labels_DF(file_paths[7] , date_path ,predictionsvebemellat)

predictionsparsian ,TestYparsian = Training(file_paths[9] , counts=6 , k=3)
parsian = Build_Labels_DF(file_paths[9] , date_path ,predictionsparsian)

predictionskhodro,TestYkhodro  = Training(file_paths[3] , counts=6 , k=3)
khodro = Build_Labels_DF(file_paths[3] , date_path ,predictionskhodro)

predictionskerooy ,TestYkerooy= Training(file_paths[8] , counts=6 , k=3)
kerooy = Build_Labels_DF(file_paths[8] , date_path ,predictionskerooy)

predictionskhesapa,TestYkhesapa  = Training(file_paths[6] , counts=6 , k=3)
khesapa = Build_Labels_DF(file_paths[6] , date_path ,predictionskhesapa)

predictionsfeloole,TestYfeloole = Training(file_paths[5] , counts=6 , k=3)
feloole = Build_Labels_DF(file_paths[5] , date_path ,predictionsfeloole)

predictionssefars,TestYsefars  = Training(file_paths[1] , counts=6 , k=3)
sefars = Build_Labels_DF(file_paths[1] , date_path ,predictionssefars)

PredictionList = [predictionsshebhern, predictionskachad , predictionsfammeli , predictionsvebemellat , predictionsparsian, predictionskhodro , predictionskerooy , predictionskhesapa , predictionsfeloole , predictionssefars]


finallist = [shebhern, kachad , fammeli , vebemellat , parsian, khodro , kerooy , khesapa , feloole , sefars]
mainLabelList =  [TestYshebhern, TestYkachad , TestYfammeli , TestYvebemellat , TestYparsian, TestYkhodro , TestYkerooy , TestYkhesapa , TestYfeloole , TestYsefars]

Final_df = pd.DataFrame({})
Final_df = pd.concat( finallist , axis=1)
Final_df.columns = ["shebhrn", "kachad" , "fammeli" , "vebemellat" , "parsian", "khodro" , "kerooy" , "khesapa" , "feloole" , "sefars"]
Final_df

Final_df.to_csv('/content/dataframe_.csv')

from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix

for (i,pred,test) in zip(range(len(Final_df.columns)),PredictionList,mainLabelList):
  conf_matrix = confusion_matrix(y_true=test , y_pred=pred)
  sns.heatmap(conf_matrix, annot=True, cmap='Blues')
  plt.title(Final_df.columns[i])
  plt.xlabel('\nPredicted Values')
  plt.ylabel('Actual Values ')
  plt.show()
  print('\n\n\n')